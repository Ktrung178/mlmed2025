# -*- coding: utf-8 -*-
"""fetal-head-hc-vgg16.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1aLIXq8uMELbdPNDtOGhyznO_-XmAtZZJ
"""

import numpy as np
import pandas as pd
import os

import torch
from torchvision import transforms, datasets, models
from torch.utils.data.dataset import Dataset
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms, utils
import torch.nn as nn
from torch import optim, cuda


import matplotlib.pyplot as plt
from skimage import io, transform


import warnings
warnings.filterwarnings("ignore")

plt.ion()

from timeit import default_timer as timer

save_file_name = 'vgg16-transfer-4.pt'
checkpoint_path = 'vgg1-transfer-4.pth'
earlyStop= 3
epochsNo= 16

root_dir='D:\\python\\lab2\\training_set\\training_set'
csv_file='D:\\python\\lab2\\training_set_pixel_size_and_HC.csv'
img_size = 224
cropped_img = 224
batch_size = 100

train_on_gpu = cuda.is_available()
print(f'Train on gpu: {train_on_gpu}')

if train_on_gpu:
    gpu_count = cuda.device_count()
    print(f'{gpu_count} gpus detected.')

HC_df=pd.read_csv(os.path.join(csv_file))

from sklearn.model_selection import train_test_split
HC_df = HC_df[['filename','pixel size(mm)','head circumference (mm)']].drop_duplicates()
HC_df=HC_df[['filename','pixel size(mm)','head circumference (mm)']].copy().rename(columns={'pixel size(mm)':'pixel_size','head circumference (mm)':'HC'})

HC_df = HC_df[HC_df.filename.str.contains("_HC")]

train_df, test_val = train_test_split(HC_df, test_size = 0.3,random_state = 2020)
valid_df, test_df = train_test_split(test_val, test_size = 0.5,random_state = 2020)


print('train', train_df.shape[0], 'validation', valid_df.shape[0],'Test',test_df.shape[0])

HC_df.describe()

HC_df['HC'].hist(bins=10)
columns = ['HC']
HC_df[columns].plot.box()
plt.xticks(rotation='vertical')

train_df.to_csv("train_file.csv", index=False, encoding='utf8')

valid_df.to_csv("val_file.csv", index=False, encoding='utf8')

test_df.to_csv("test_file.csv", index=False, encoding='utf8')

from PIL import Image

class HC_18(Dataset):

    def __init__(self, csv_file, root_dir, transform=None):

        self.HC_df = pd.read_csv(csv_file)
        self.root_dir = root_dir
        self.transform = transform

    def __len__(self):
        return len(self.HC_df)

    def __getitem__(self, idx):
        if torch.is_tensor(idx):
            idx = idx.tolist()

        img_name = os.path.join(self.root_dir,self.HC_df.iloc[idx, 0])
        image = Image.open(img_name)
        features = self.HC_df.loc[idx,'HC']

        if self.transform:
            image = self.transform(image)
        return image , torch.tensor(features / 346.4)

"""# **Images Transformations**"""

image_transforms = {
    'train':
        transforms.Compose([
            transforms.Grayscale(num_output_channels=3),
            transforms.Resize(size=img_size),
            transforms.RandomRotation(degrees=10),
            transforms.RandomHorizontalFlip(),
            transforms.CenterCrop(size=cropped_img),
            transforms.ToTensor(),
            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
        ]),

    'val':
        transforms.Compose([
            transforms.Grayscale(num_output_channels=3),
            transforms.Resize(size=img_size),
            transforms.CenterCrop(size=cropped_img),
            transforms.ToTensor(),
            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
        ]),
    'test':
        transforms.Compose([
            transforms.Grayscale(num_output_channels=3),
            transforms.Resize(size=img_size),
            transforms.CenterCrop(size=cropped_img),
            transforms.ToTensor(),
            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
        ])}

train_filename='train_file.csv'
val_filename='val_file.csv'
test_filename='test_file.csv'

data = {'train': HC_18(train_filename ,root_dir = root_dir,transform = image_transforms['train']),
        'val': HC_18(val_filename ,root_dir = root_dir,transform = image_transforms['val']),
        'test': HC_18(test_filename ,root_dir = root_dir,transform = image_transforms['test'])
       }

dataloaders ={
    'train': DataLoader(data['train'], batch_size=batch_size,shuffle=True),
    'val': DataLoader(data['val'], batch_size=batch_size,shuffle=False),
    'test': DataLoader(data['test'], batch_size=len(test_df),shuffle=False)
}

model = models.vgg16(pretrained=True)
n_inputs = model.classifier[0].in_features
model.classifier = nn.Sequential(
    nn.Dropout(0.5),
    nn.Linear(n_inputs, 1 , bias = True))

print(model)

if train_on_gpu:
    model = model.to('cuda')

def train(model, criterion, optimizer, train_loader, valid_loader, save_file_name, max_epochs_stop=3, n_epochs=20, print_every=1):
    epochs_no_improve = 0
    valid_loss_min = np.inf
    history = []
    try:
        print(f'Model trained for: {model.epochs} epochs.\n')
    except:
        model.epochs = 0
        print(f'Starting training...\n')

    overall_start = timer()
    for epoch in range(n_epochs):
        train_loss, valid_loss, train_mae, valid_mae = 0.0, 0.0, 0.0, 0.0
        model.train()
        start = timer()

        for ii, (data, target) in enumerate(train_loader):
            if train_on_gpu: data, target = data.cuda(), target.cuda()
            optimizer.zero_grad()
            output = model(data)
            loss = criterion(output, target.float())
            loss.backward()
            optimizer.step()
            train_loss += loss.item() * data.size(0)
            train_mae += torch.nn.L1Loss()(output, target.float()).item() * data.size(0)
            print(f'Epoch: {epoch+1}\t{100 * (ii + 1) / len(train_loader):.2f}% complete. {timer() - start:.2f}s', end='\r')

        model.epochs += 1
        model.eval()
        with torch.no_grad():
            for data, target in valid_loader:
                if train_on_gpu: data, target = data.cuda(), target.cuda()
                output = model(data)
                valid_loss += criterion(output, target.float()).item() * data.size(0)
                valid_mae += torch.nn.L1Loss()(output, target.float()).item() * data.size(0)

        train_loss /= len(train_loader.dataset)
        valid_loss /= len(valid_loader.dataset)
        train_mae /= len(train_loader.dataset)
        valid_mae /= len(valid_loader.dataset)
        history.append([train_loss, valid_loss, train_mae, valid_mae])

        if (epoch + 1) % print_every == 0:
            print(f'\nEpoch: {epoch+1} \tTrain Loss: {train_loss:.4f} \tValid Loss: {valid_loss:.4f} \tTrain MAE: {train_mae:.4f} \tValid MAE: {valid_mae:.4f}')

        if valid_loss < valid_loss_min:
            torch.save(model.state_dict(), save_file_name)
            epochs_no_improve = 0
            valid_loss_min = valid_loss
            best_epoch = epoch
        else:
            epochs_no_improve += 1
            if epochs_no_improve >= max_epochs_stop:
                print(f'\nEarly Stopping! Epochs: {epoch+1}. Best epoch: {best_epoch+1} with loss: {valid_loss_min:.2f}')
                total_time = timer() - overall_start
                print(f'{total_time:.2f}s total. {total_time / (epoch+1):.2f}s/epoch.')
                model.load_state_dict(torch.load(save_file_name))
                model.optimizer = optimizer
                history_df = pd.DataFrame(history, columns=['train_loss', 'valid_loss', 'train_mae', 'valid_mae'])
                return model, history_df

    model.optimizer = optimizer
    total_time = timer() - overall_start
    print(f'\nBest epoch: {best_epoch+1} with loss: {valid_loss_min:.2f}')
    print(f'{total_time:.2f}s total. {total_time / (epoch + 1):.2f}s/epoch.')
    history_df = pd.DataFrame(history, columns=['train_loss', 'valid_loss', 'train_mae', 'valid_mae'])
    return model, history_df

criterion = nn.SmoothL1Loss(beta = 0.5)
optimizer = optim.Adam(model.parameters(),lr=0.0001)

model, history = train(model,criterion,optimizer,dataloaders['train'],dataloaders['val'],save_file_name=save_file_name,max_epochs_stop=earlyStop,n_epochs=epochsNo,print_every=1)
history.to_csv("history1.csv", index=False, encoding='utf8')

"""# **Results Graph**"""

plt.figure(figsize=(10, 8))
for c in ['train_loss', 'valid_loss']:
    plt.plot(history[c], label=c)
plt.legend()
plt.xlabel('Epoch',color='k')
plt.ylabel('Average Negative Log Likelihood')
plt.title('Training and Validation Losses')

plt.figure(figsize=(10, 8))
for c in ['train_mae', 'valid_mae']:
    plt.plot(history[c], label=c)
plt.legend()
plt.xlabel('Epoch',color='k')
plt.ylabel('Average Negative Log Likelihood')
plt.title('MAEMAE')

def save_checkpoint(model, path):

    checkpoint = {
                'epochs': model.epochs
                 }

    checkpoint['classifier'] = model.classifier
    checkpoint['state_dict'] = model.state_dict()

    checkpoint['optimizer'] = model.optimizer
    checkpoint['optimizer_state_dict'] = model.optimizer.state_dict()
    torch.save(checkpoint, path)

save_checkpoint(model, path=checkpoint_path)

def check_mse_on_test(model, testloader, criterion=None, device='cpu'):

    loss = 0
    test_loss = 0
    std_mean = 0
    model.eval()
    test_dataframe = pd.read_csv(test_filename)
    with torch.no_grad():
        for data,target in testloader:
            if train_on_gpu:
                data, target = data.cuda(), target.cuda()
            output = model(data)

            test_dataframe ['predicted output'] = np.squeeze(output.cpu().numpy())
            test_dataframe.to_csv("D:\\python\\lab2\\test_file55.csv", index=False, encoding='utf8')

            std_mean = torch.std_mean(output)
            loss = criterion(output, target.float())
            test_loss += loss.item() * data.size(0)

    return std_mean , test_loss / len(testloader.dataset)

criterion = nn.L1Loss()

std_mean , test_loss = check_mse_on_test(model, dataloaders['test'], criterion, device='cuda')
print('Test set MAE loss = {:.4f} '.format(test_loss))
print('Test set standard deviation of Hc =  {:.4f} mm'.format(std_mean[0] * 346.4))
print('Test set MAE loss of HC = {:.4f} mm'.format(test_loss * 346.4))