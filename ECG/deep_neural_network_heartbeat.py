# -*- coding: utf-8 -*-
"""deep-neural-network-heartbeat.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/136fatD46eTG6Jk0FL0dLjLv1qN5u0619
"""

import numpy as np
import pandas as pd
import tensorflow as tf
import matplotlib.pyplot as plt
from tensorflow import keras
from tensorflow.keras import layers
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

from sklearn.preprocessing import StandardScaler

from keras.models import Sequential
from keras.layers import Dense
from keras.optimizers import Adam

train_data1 = pd.read_csv(r'/home/cubid/code/tf17/ECG/archive/mitbih_train.csv',header=None)
train_data1.head()

test_data1 = pd.read_csv(r'/home/cubid/code/tf17/ECG/archive/mitbih_test.csv',header=None)
test_data1.head()

X_train, y_train = train_data1.iloc[:, :-1].values, train_data1.iloc[:, -1].values
X_test, y_test = test_data1.iloc[:, :-1].values, test_data1.iloc[:, -1].values

from collections import Counter
import matplotlib.pyplot as plt
import seaborn as sns

# H√†m hi·ªÉn th·ªã t·∫ßn su·∫•t c√°c l·ªõp
def plot_class_distribution(y, title):
    counter = Counter(y)
    classes = list(counter.keys())
    counts = list(counter.values())

    plt.figure(figsize=(8, 5))
    sns.barplot(x=classes, y=counts, palette="viridis")
    plt.xlabel("Class Labels")
    plt.ylabel("Frequency")
    plt.title(title)
    plt.show()

# T·∫ßn su·∫•t t·ª´ng l·ªõp trong t·∫≠p train tr∆∞·ªõc khi c√¢n b·∫±ng
plot_class_distribution(y_train, "Class Distribution Before Balancing")

# üîÑ **C√¢n b·∫±ng d·ªØ li·ªáu b·∫±ng Oversampling (SMOTE)**
from imblearn.over_sampling import SMOTE

smote = SMOTE(random_state=42)
X_train_bal, y_train_bal = smote.fit_resample(X_train.reshape(X_train.shape[0], -1), y_train)

# Reshape l·∫°i th√†nh d·∫°ng (samples, timesteps, features)
X_train_bal = X_train_bal.reshape((X_train_bal.shape[0], 1, X_train_bal.shape[1]))

# T·∫ßn su·∫•t t·ª´ng l·ªõp sau khi c√¢n b·∫±ng
plot_class_distribution(y_train_bal, "Class Distribution After Balancing")

# T·∫ßn su·∫•t t·ª´ng l·ªõp trong t·∫≠p validation v√† test
plot_class_distribution(y_test, "Class Distribution in Validation & Test Set")

print(f"train data size: {train_data1.shape}")
print(f"test data size: {test_data1.shape}")

print(train_data1.isna().sum())

X_train = train_data1.iloc[:, :-1]
y_train = train_data1.iloc[:, -1]

print("Unique values in y:", y_train.unique())

X_test = test_data1.iloc[:, :-1]
y_test = test_data1.iloc[:, -1]

scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

X_train.shape

X_test.shape

from keras.models import Sequential
from keras.layers import Dense, Dropout, BatchNormalization
from keras.optimizers import Adam
from keras.callbacks import ReduceLROnPlateau, EarlyStopping
from keras.regularizers import l2

# Build the model
model = Sequential()
model.add(Dense(256, input_dim=X_train.shape[1], activation='relu', kernel_regularizer=l2(0.001)))
model.add(BatchNormalization())
model.add(Dropout(0.3))
model.add(Dense(128, activation='relu', kernel_regularizer=l2(0.001)))
model.add(BatchNormalization())
model.add(Dropout(0.3))
model.add(Dense(64, activation='relu', kernel_regularizer=l2(0.001)))
model.add(BatchNormalization())
model.add(Dropout(0.3))
model.add(Dense(len(np.unique(y_train)), activation='softmax'))

model.compile(optimizer=Adam(learning_rate=0.001),
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)
lr_reduction = ReduceLROnPlateau(monitor='val_loss', patience=3, factor=0.5, min_lr=1e-6)

history = model.fit(X_train, y_train,
                    epochs=30,
                    batch_size=128,
                    validation_data=(X_test, y_test),
                    callbacks=[lr_reduction, early_stopping],
                    verbose=1)

loss, accuracy = model.evaluate(X_test, y_test, verbose=0)
print(f'Test Loss: {loss:.4f}')
print(f'Test Accuracy: {accuracy:.4f}')

from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np

# üü¢ **D·ª± ƒëo√°n tr√™n t·∫≠p test**
y_pred = model.predict(X_test)
y_pred_classes = np.argmax(y_pred, axis=1)  # Chuy·ªÉn sang nh√£n d·ª± ƒëo√°n

# üìå **Classification Report (Precision, Recall, F1-Score)**
print("üìã Classification Report:")
print(classification_report(y_test, y_pred_classes, digits=4))

# üìä **Confusion Matrix**
cm = confusion_matrix(y_test, y_pred_classes)
labels = sorted(np.unique(y_test))  # L·∫•y danh s√°ch nh√£n theo th·ª© t·ª±

# üîπ **V·∫Ω Confusion Matrix**
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels)
plt.xlabel("Predicted Label")
plt.ylabel("True Label")
plt.title("Confusion Matrix")
plt.show()

# üöÄ **Hi·ªÉn th·ªã Confusion Matrix d∆∞·ªõi d·∫°ng tr·ª±c quan**
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)
disp.plot(cmap="Blues")
plt.show()

from sklearn.metrics import classification_report

y_pred = model.predict(X_test)
y_pred = np.argmax(y_pred, axis=1)

report = classification_report(y_test, y_pred)
print(report)

"""# LSTM

"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.preprocessing import StandardScaler

from keras.models import Sequential
from keras.layers import LSTM, Dense, Dropout, BatchNormalization
from keras.optimizers import Adam
from keras.callbacks import ReduceLROnPlateau, EarlyStopping
from keras.regularizers import l2

train_data1 = pd.read_csv('/home/cubid/code/tf17/ECG/archive/mitbih_train.csv', header=None)
test_data1 = pd.read_csv('/home/cubid/code/tf17/ECG/archive/mitbih_test.csv', header=None)

from collections import Counter
import matplotlib.pyplot as plt
import seaborn as sns

# H√†m hi·ªÉn th·ªã t·∫ßn su·∫•t c√°c l·ªõp
def plot_class_distribution(y, title):
    counter = Counter(y)
    classes = list(counter.keys())
    counts = list(counter.values())

    plt.figure(figsize=(8, 5))
    sns.barplot(x=classes, y=counts, palette="viridis")
    plt.xlabel("Class Labels")
    plt.ylabel("Frequency")
    plt.title(title)
    plt.show()

# T·∫ßn su·∫•t t·ª´ng l·ªõp trong t·∫≠p train tr∆∞·ªõc khi c√¢n b·∫±ng
plot_class_distribution(y_train, "Class Distribution Before Balancing")

# üîÑ **C√¢n b·∫±ng d·ªØ li·ªáu b·∫±ng Oversampling (SMOTE)**
from imblearn.over_sampling import SMOTE

smote = SMOTE(random_state=42)
X_train_bal, y_train_bal = smote.fit_resample(X_train.reshape(X_train.shape[0], -1), y_train)

# Reshape l·∫°i th√†nh d·∫°ng (samples, timesteps, features)
X_train_bal = X_train_bal.reshape((X_train_bal.shape[0], 1, X_train_bal.shape[1]))

# T·∫ßn su·∫•t t·ª´ng l·ªõp sau khi c√¢n b·∫±ng
plot_class_distribution(y_train_bal, "Class Distribution After Balancing")

# T·∫ßn su·∫•t t·ª´ng l·ªõp trong t·∫≠p validation v√† test
plot_class_distribution(y_test, "Class Distribution in Validation & Test Set")

X_train = train_data1.iloc[:, :-1]
y_train = train_data1.iloc[:, -1]
X_test = test_data1.iloc[:, :-1]
y_test = test_data1.iloc[:, -1]

# Chu·∫©n h√≥a d·ªØ li·ªáu
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Reshape d·ªØ li·ªáu th√†nh (samples, timesteps, features)
X_train = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))
X_test = X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))

model = Sequential()
model.add(LSTM(128, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2])))
model.add(BatchNormalization())
model.add(Dropout(0.3))
model.add(LSTM(64, return_sequences=False))
model.add(BatchNormalization())
model.add(Dropout(0.3))
model.add(Dense(32, activation='relu', kernel_regularizer=l2(0.001)))
model.add(BatchNormalization())
model.add(Dropout(0.3))
model.add(Dense(len(np.unique(y_train)), activation='softmax'))

# Compile m√¥ h√¨nh
model.compile(optimizer=Adam(learning_rate=0.001),
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

# Callbacks
early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)
lr_reduction = ReduceLROnPlateau(monitor='val_loss', patience=3, factor=0.5, min_lr=1e-6)

history = model.fit(X_train, y_train,
                    epochs=25,
                    batch_size=128,
                    validation_data=(X_test, y_test),
                    callbacks=[lr_reduction, early_stopping],
                    verbose=1)

# ƒê√°nh gi√° m√¥ h√¨nh
loss, accuracy = model.evaluate(X_test, y_test, verbose=0)
print(f'Test Loss: {loss:.4f}')
print(f'Test Accuracy: {accuracy:.4f}')

y_pred = model.predict(X_test)
y_pred_classes = np.argmax(y_pred, axis=1)

# B√°o c√°o k·∫øt qu·∫£
report = classification_report(y_test, y_pred_classes)
print(report)

from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np

# üü¢ **D·ª± ƒëo√°n tr√™n t·∫≠p test**
y_pred = model.predict(X_test)
y_pred_classes = np.argmax(y_pred, axis=1)  # Chuy·ªÉn sang nh√£n d·ª± ƒëo√°n

# üìå **Classification Report (Precision, Recall, F1-Score)**
print("üìã Classification Report:")
print(classification_report(y_test, y_pred_classes, digits=4))

# üìä **Confusion Matrix**
cm = confusion_matrix(y_test, y_pred_classes)
labels = sorted(np.unique(y_test))  # L·∫•y danh s√°ch nh√£n theo th·ª© t·ª±

# üîπ **V·∫Ω Confusion Matrix**
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels)
plt.xlabel("Predicted Label")
plt.ylabel("True Label")
plt.title("Confusion Matrix")
plt.show()

# üöÄ **Hi·ªÉn th·ªã Confusion Matrix d∆∞·ªõi d·∫°ng tr·ª±c quan**
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)
disp.plot(cmap="Blues")
plt.show()